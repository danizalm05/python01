
"""95-DL_terminology_loss_functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FAN1kWedRsl_iiFFwSEGAtpC6UYhcaVX

95_DL_terminology_loss_functions.ipynb
--------------------------------------------
Common loss functions explaind using simple examples


Training Video:
https://youtu.be/FiXMj0Jlihk
https://github.com/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial95_DL_terminology_loss_functions.ipynb
"""

# Mean squared error - used in regression problems
# Average of squared distances between actual and predicted values
# This loss value is minimized during the training process of a neural network

actual = [1.2, 2.3, 3, 4.5, 5.2, 6.3, 6.9, 8.4, 9.1, 9.9]
predicted1 = [1.1, 2.1, 3.3, 4.2, 4.9, 6., 7.2, 8., 9.5, 10.2]
predicted2 = [0.8, 2., 2.6, 4.8, 5.7, 6.1, 6.5, 7.8, 9.5, 9.6]

def mean_squared_error(actual, predicted):
	sum_square_error = 0.0
	for i in range(len(actual)):
		sum_square_error += (actual[i] - predicted[i])**2.0
	mean_square_error = 1.0 / len(actual) * sum_square_error
	return mean_square_error

print(mean_squared_error(actual, predicted1))
print(mean_squared_error(actual, predicted2))

# cross entropy (binary) - also referred to as log loss
# Used for binary classification problems. Can be extended to multiclass.
# Logarthimic penalty based on the comparison of actual and predicted classes.
# Perfect prediction means 0 cross entropy.
from numpy import mean
from math import log

actual_label = [0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1]
predicted1 = [0.1, 0.9, 0.8, 0.7, 0.3, 0.2, 0.9, 0.4, 0.9, 0.99, 0.1, 0.8]
predicted2 = [0.9, 0.1, 0.8, 0.7, 0.3, 0.2, 0.2, 0.4, 0.1, 0.99, 0.1, 0.6]

# calculate cross entropy
def cross_entropy(p, q):     #p is actual and q is predicted value
	return -sum([p[i]*log(q[i]) for i in range(len(p))])

#Calculating cross entropy for each pair of actual vs predicted
predicted_prob = predicted2  #Test predicted 1 and 2

results = list()
for i in range(len(actual_label)):
  #Create list for each occurance (binary --> [Not, actual]). Can be extended to multi class.
  actual_value = [1.0 - actual_label[i], actual_label[i]]
  #print(actual_value)
  #Create list for each occurance (binary --> [Not, predicted]). Can be extended to multi class.
  predicted_value = [1.0 - predicted_prob[i], predicted_prob[i]]
  #print(predicted_value)

  # calculate cross entropy:
  ce = cross_entropy(actual_value, predicted_value)
  print('actual=%.1f, predicted=%.1f --- cross_entropy: %.3f ' % (actual_label[i], predicted_prob[i], ce))
  results.append(ce)

# calculate the average cross entropy
mean_ce = mean(results)
print('Average Cross Entropy: ', mean_ce)

# Use this guide to assess the loss during your training process
#Cross-Entropy = 0.00: Perfect Match.
#              < 0.02: Great match.
#              < 0.05: Good.
#              < 0.20: Acceptable.
#              > 0.30: Not good.
#              > 1.00: Bad.
#              > 2.00 Horrible.




import numpy as np

import tensorflow.keras.backend as K
#from keras import backend
from keras.losses import binary_crossentropy

actual_label = np.array([[0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [1, 1, 0, 0, 1]])

#Same as actual_label
predicted = np.array([[0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [1, 1, 0, 0, 1]], dtype=np.float32)

#Class 1: Close to actual
#Class 2: 1 misclassification (0.4 instead of 1, 2 in the middle at 0.5)
#Class 3: 4 misclassifications
predicted1 = np.array([[0.1, 0.8, 0.2, 0.9, 0.1], [0.4, 0.5, 0.5, 0.3, 0.7], [0.1, 0.2, 0.7, 0.6, 0.5]], dtype=np.float32)

#Class 1: 3 misclassifications
#Class 2: 0 misclassifications
#Class 3: 5 misclassifications
predicted2 = np.array([[0.8, 0.3, 0.8, 0.8, 0.4], [0.6, 0.2, 0.8, 0.1, 0.6], [0.1, 0.3, 0.8, 0.9, 0.1]], dtype=np.float32)

# Convert the arrays to keras variables
#y_true = K.constant(actual_label)
'''
y_true = K.eval(K.constant(actual_label))
y_pred = K.eval(K.constant(predicted))
y_pred1 = K.eval(K.constant(predicted1))
y_pred2 = K.eval(K.constant(predicted2))
 '''

y_true = K.variable(actual_label)
y_pred = K.variable(predicted)
y_pred1 = K.variable(predicted1)
y_pred2 = K.variable(predicted2)

# calculate mean cross-entropy
mean_ce = K.eval(binary_crossentropy(y_true, y_pred))
mean_ce1 = K.eval(binary_crossentropy(y_true, y_pred1))
mean_ce2 = K.eval(binary_crossentropy(y_true, y_pred2))
print('Average Cross Entropy for pred: ', mean_ce)
print('Average Cross Entropy for pred1: ', mean_ce1)
print('Average Cross Entropy for pred2: ', mean_ce2)

#In general the loss is high for all classes even though some classes are easier to classify.
#See loss for class 2 in pred1, we see a high value of 0.6 despite having results close to actual.
# Focal loss addresses this issue.

#Focal loss
#from keras import backend
import tensorflow as tf
import numpy as np

actual_label = np.array([[0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [1, 1, 0, 0, 1]])

#Class 1: Close to actual
#Class 2: 1 misclassification (0.4 instead of 1, 2 in the middle at 0.5)
#Class 3: 4 misclassifications
predicted1 = np.array([[0.1, 0.8, 0.2, 0.9, 0.1], [0.4, 0.5, 0.5, 0.3, 0.7], [0.1, 0.2, 0.7, 0.6, 0.5]], dtype=np.float32)

# Convert the arrays to keras variables
y_true = K.variable(actual_label)
y_pred1 = K.variable(predicted1)

# Focal loss function
gamma=3.  #When gamma=0, focal loss should be same as cross entropy. Try gamma=3. Higher gamma adds more weight to misclassified classes.
alpha=0.25  #
def focal_loss_fixed(y_true, y_pred):

	pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
	pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
	return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))

c=1  #Class
FL_class1 = focal_loss_fixed(y_true[0], y_pred1[0])
FL_class2 = focal_loss_fixed(y_true[1], y_pred1[1])
FL_class3 = focal_loss_fixed(y_true[2], y_pred1[2])

print('Average Focal Loss for class1: ', FL_class1)
print('Average Focal Loss for class2: ', FL_class2)
print('Average Focal Loss for class3: ', FL_class3)

#When gamma=0, FL=CL. It is clear that the loss for second class is high eventhough it is classifying almost ok.
#When gamma=3, The FL for class 2 is small and yet the loss for class 3 is about 7 times higher than that of class 2.
#This makes the network focus more on class 3 during training ensuring better results for this class against easy to classify classes.