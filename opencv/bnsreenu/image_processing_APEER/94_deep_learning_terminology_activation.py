
"""94 - Deep Learning terminology  - Activation
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1wAR5Las0z_BmDE7XtdSne-BKM69u9Ivg

94 - Deep Learning terminology explained -
                    Activation
--------------------------------------------


Suggestions:

   1 Sigmoid works well for classifiers
   2 Sigmoid (and tanh) are usually avoided
       in hidden layers due to the vanishing
        gradient problem
   3 ReLU is often used in hidden layers,
         especially for CNNs.
   5 Never use ReLU outside the hidden layers.
    Use softmax for multiclass classification
Training Video:
https://www.youtube.com/watch?v=vp8mQ_inplo&list=PLHae9ggVvqPgyRQQOtENr6hK0m1UquGaG&index=98
https://github.com/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial94_DL_terminology_Activation.ipynb

# **Code to explain Activation Functions**

**Example use case** - A family deciding on the cuisine to eat out. Choices are, Thai, Sushi and Indian.

Each family member has weight associated with their pwoer to influence the decision.

Mom: 0.5
Dad: 0.2
Son: 0.3

Daughter: 0.6

Each member provides a rating for each cuisine type that ranges between -100 (really hate) to 100 (really love).

Ratings for each cusine given by Mom, da, son, and daughter, respectively, in that order.

Thai: -30, 100, 40, -40

Sushi: -10, -25, -60, 50

Indian: -50, -50, 50, 40

Weighted sum as calculated by the neuron for each cuisine type by adding the product of above weights and ratings by each family member:

Thai: -7

Sushi: 2

Indian: 4

These will be used as inputs to each activation function velow.
"""

import numpy as np
my_data = np.array([-7, 2, 4])  #Thai, Sushi, Indian, respectively.

# Linear kernel. Input values with some
# scaling constant  Gradient (derivative)
# will be a constant.
# This means the update factor for weights
# and biases during training is the same.
# The network will not be improving better
# during training, especially for complex
# scenarios.

def linear(x):
    return 2*x  #Scaling constant = 2

linear(my_data)

#Step function. 0 for all values below 0 and 1 for all values above 0.
# Useful for binary classification and useless for multiclass classification.
# Gradient of step function is zero, so do not use this for hidden layers as the weights will not be updated.
# Use this for the classification layer.

def step(x):
    if x<0:
        return 0
    else:
        return 1
   

print(step(-7))
print(step(2))
print(step(4))

#sigmoid function - transforms input values to values between 0 and 1
# Continuously differentialble function. The derivative tapers off (gets small) pretty fast, around values -4 / 4.
# This means the network is not learning when these limits are reached.
# Output values do not all add up to 1.

def sigmoid(X):
   return 1/(1+np.exp(-X))

sigm_result = sigmoid(my_data)

print(sigm_result)

sigm_result_binarized = sigm_result>0.5
print(sigm_result_binarized)

#tanh(x)=2sigmoid(2x)-1
# Similar to sogmoid except this is symmetric around origin and ranges from -1 to 1.
# Usually tanh is preferred over the sigmoid function as it is zero centered

def tanh(x):
    return (2/(1 + np.exp(-2*x))) -1

tanh_result=tanh(my_data)

print(tanh_result)

#ReLu - Rectified Linear Unit
# Neurons will only be deactivated if the
# output of the transformation is less than 0.
# Output is 0 for negative values and linear
# for values above 0.
# negative side og the derivative (gradient)
# is 0, this may lead to dead neurons that
# never get activated.
# Leaky ReLU can be used if this is a
# problem.
def relu(X):
   return np.maximum(0,X)

relu(my_data)

# Leaky ReLu
#Similar to reLU except for values below zero instead of output being 0 it is a tiny linear component.
def leaky_relu(x):
    if x<0:
        return 0.1*x
    else:
        return x
print(leaky_relu(-7))
print(leaky_relu(2))
print(leaky_relu(4))

#Softmax is like combining multiple sigmoids.
# Softmax is used for multiclass classification problems.
# Output returns the probability for a datapoint belonging to each individual class.
#All probabilities add to 1

def softmax(X):
    expo = np.exp(X)
    expo_sum = np.sum(np.exp(X))
    return expo/expo_sum

#Example with mmatrix defined above
a, b, c = softmax(my_data)
print (softmax(my_data))
print("The sum of all values = ", a+b+c)









''' Normalizer
Normalize samples individually to unit norm.

Each sample (i.e. each row of the data matrix)
with at least one non zero component is rescaled
independently of other samples so that its
norm (l1, l2 or inf) equals one.

Scaling inputs to unit norms is a common
operation for text classification or
clustering for instance. For instance the
dot product of two l2-normalized TF-IDF
vectors is the cosine similarity of the
vectors and is the base similarity metric
for the Vector Space Model commonly used by
the Information Retrieval community.
'''
Normalized = Normalizer(norm='l1')  #Norm = l1 or l2 or max
Normalized_X = pd.DataFrame(Normalized.fit_transform(X.values), columns=X.columns, index=X.index)

fig, ax =plt.subplots(1,2, figsize=(15, 5))
ax[0].set_title('Original Data')
sns.scatterplot(data=X, x="area_mean", y="smoothness_se", ax=ax[0])
ax[1].set_title('After Normalization')
sns.scatterplot(data=Normalized_X, x="area_mean", y="smoothness_se", ax=ax[1])







#plot the training and validation accuracy and loss at each epoch
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()



acc = history.history['accuracy']  #Use accuracy if acc doesn't work
val_acc = history.history['val_accuracy']  #Use val_accuracy if acc doesn't work
plt.plot(epochs, acc, 'y', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#    41:12
#youtu.be/kbkLLPcyU-Q
#github.com/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial91_binary_classification_breast_cancer.ipynb

# Predicting the Test set results
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True)